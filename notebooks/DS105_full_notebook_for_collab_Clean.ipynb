{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS105_full_notebook_for_collab_Clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_dWYnIrXKqOV"
      },
      "source": [
        "# Setup\n",
        "\n",
        "First let's install and setup the necessary libraries.\n",
        "\n",
        "First clone the github repository with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MV3AKSEPKqOy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020.git\n",
        "#With this command, the path to the data is \n",
        "workdir = './CrossLingual-NLP-AMLD2020/'\n",
        "os.environ[\"WORKDIR\"] = workdir\n",
        "\n",
        "#Please check if this correct, otherwise correct path_to_data\n",
        "!printenv WORKDIR\n",
        "!ls $WORKDIR/data/laser\n",
        "!mkdir $WORKDIR/data/raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "38uYYYfCDz55"
      },
      "source": [
        "Download data from on your local file system and upload it to colab fs with the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ilYMXGhaxIJf",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# !tar -jxf *.bz2 -C  $WORKDIR/data/raw/\n",
        "# !rm ./semeveal15_sentiment_datasets.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "802iKv8-taLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TY_COMMENT\n",
        "# above cell does not work with safari.\n",
        "# either switch to chrome \n",
        "# or upload the gz2 file using \"upload\" button on the side bar \n",
        "# then do the following:\n",
        "\n",
        "!tar -jxf *.bz2 -C  $WORKDIR/data/raw/\n",
        "!rm ./semeveal15_sentiment_datasets.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-e5eYKMQ-zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure you have all the semeval files \n",
        "!ls $WORKDIR/data/raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8W3-_XJZFA9s"
      },
      "source": [
        "Install LASER and conceptNet.  **This might take a \n",
        "while**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ksFtp-qvKqPO",
        "colab": {}
      },
      "source": [
        "%cd CrossLingual-NLP-AMLD2020/\n",
        "!bash install_laser.sh\n",
        "!bash download_conceptNet.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcegNHZPKqPo"
      },
      "source": [
        "Restart the runtime  environnement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S4jhV-y5DHSF",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QKgzMiOYDPPD"
      },
      "source": [
        "Set  environnement variables and load modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TzJZ79Y4KqPu",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "os.environ.setdefault(\"LASER\",\"/root/projects/LASER/\")\n",
        "assert os.environ.get('LASER'), 'Please set the environement variable LASER'\n",
        "LASER = os.environ['LASER']\n",
        "sys.path.append(LASER + 'source/lib')\n",
        "sys.path.append(LASER+\"source/\")\n",
        "\n",
        "workdir = './CrossLingual-NLP-AMLD2020/'\n",
        "os.environ[\"WORKDIR\"] = workdir\n",
        "sys.path.insert(1, workdir)\n",
        "\n",
        "from src.models import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn9ru80gSns3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!printenv WORKDIR\n",
        "!printenv LASER"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8vNOIVQOKqP3"
      },
      "source": [
        "If everything went well the following should not print any errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7EhaiKvfKqP5",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from src.models import *\n",
        "\n",
        "print(Doc2Laser.__doc__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh9pQNDzvs3B",
        "colab_type": "text"
      },
      "source": [
        "Yay!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nvnCwzJpKqQH"
      },
      "source": [
        "# Introduction to Language Representation and Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZlYPmddnZiU",
        "colab_type": "text"
      },
      "source": [
        "## Language is hard! \n",
        "\n",
        "Take a look at the following sentences: \n",
        "1. Jane went to the store\n",
        "2. went the to Jane store \n",
        "3. Jane went store \n",
        "4. Jane goed store \n",
        "\n",
        "They (try to) express similar meanings, but some feel un-natural!  \n",
        "\n",
        "Several things to handle: \n",
        "- Morphology\n",
        "- Syntax <- touch on this \n",
        "- Semantics/World Knowledge <- touch on this but mostly shallow semantics\n",
        "- Discourse \n",
        "- Pragmatics \n",
        "- Multilinguality <- focus on this\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRxsztDbn0YK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Sentiment Classification\n",
        "- A type of Text Classification task.\n",
        "- binary (positive, negative)\n",
        "- ternary (positive, neutral, negative)\n",
        "- ordinal (image below!)\n",
        "\n",
        "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/sentiment-5class.png?raw=1\" width=\"600\">\n",
        "\n",
        "*Input* (x): a text span \n",
        "\n",
        "*Output* (y): a class/category (sentiment polarity in the sentiment classification example)\n",
        "\n",
        "**Goal**: Train a function $f(x) \\rightarrow y$\n",
        "\n",
        "- How to represent text? \n",
        "- What functions can we use for the task? \n",
        "- How to evaluate performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ycmXbYllKqQK"
      },
      "source": [
        "\n",
        "## Machine Learning workflow\n",
        "1. Get data\n",
        "1. Inspect the data\n",
        "1. Preprocess/Clean/Normalize the data\n",
        "1. Vector Representation\n",
        "1. Modeling \n",
        "1. Evaluation\n",
        "\n",
        "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/pipeline.png?raw=1\" width=\"600\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSEN-u6XoCjd",
        "colab_type": "text"
      },
      "source": [
        "## Text representation: Bag-of-Words and TF/IDF weiging\n",
        "\n",
        "Given a text, extract the vocabulary, build a vector of dim $|V|$, non-zeros are words that appear. \n",
        "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/textVectorization.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4cGT-e6ZKqQN",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['This is the first document.',\n",
        "'This document is the second document.',\n",
        "'And this is the third one.',\n",
        "'Is this the first document?']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PE0-XzS9KqQT"
      },
      "source": [
        "\n",
        "- Words are identified by their ids\n",
        "- Non-zero means a word occurs\n",
        "- The value, is the number of times the word occurs in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CQXI6Zb-KqQV",
        "colab": {}
      },
      "source": [
        "vectorizer.transform(['This is the first document', 'is document the first this']).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KUtSI3qKKqQd"
      },
      "source": [
        "- order does not matter! Recall the example with Jane ;-)\n",
        "- words like 'and, the' matter the same with words like 'super, great, ..'. This is a limitation. \n",
        "- tf-idf (term frequence, inverse document frequency) is an heuristic that can get us far!\n",
        "\n",
        "$tf_{i,j}\\times\\log\\frac{N}{df_i}$\n",
        "\n",
        "where\n",
        "\n",
        "$tf_{i,j}$ is number of times the term $i$ appears in document $j$, $df_i$ is the document frequency in the full collection of documents and $N$ is the number of available documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dqL2Nl-eKqQf",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = ['This is the first document.',\n",
        "'This document is the second document.',\n",
        "'And this is the third one.',\n",
        "'Is this the first document?']\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eo2hvI49KqQm"
      },
      "source": [
        "## Beyond single words: N-grams and Character-grams\n",
        "\n",
        "Other tricks and tips: \n",
        "- Recall text is a sequence of symbols. We may care for characters instead of words (think typos) \n",
        "- We may care for longer sequences that single words: New York, not great, .. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CB6cHvsKqQo",
        "colab": {}
      },
      "source": [
        "# Character grams\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,1)) # This creates character-grams of size 1\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cn-X1KOUKqQx"
      },
      "source": [
        "N-grams are sequences of *objects*. Here, objects, are either charactets sequences or word sequences. For character sequences for example:\n",
        "\n",
        "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/ngrams.png?raw=1\">\n",
        "\n",
        "In this figure notice the sliding window of size 3. While moving from left to right, it generates the possible sequences that will be used to populate the vector representations. Due to the fact that the window is of size 3, the method will generate character 3-grams. If, instead of character in the figure, we were using words, we would be generating word 3-grams. \n",
        "\n",
        "**Question**: can you think of a limitation of word 3-grams, 4-grams, 5-grams etc.?\n",
        "\n",
        "**Exercise**: how to get these sequences in Python (in an elegant way)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cRrqNh1NKqQ0",
        "colab": {}
      },
      "source": [
        "# N-grams (can be either char-grams or word-grams)\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2,3)) # This creates character-grams of sizes 2 and 3\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YcUkxRjKqQ5"
      },
      "source": [
        "# Dense Representations\n",
        "\n",
        "All of the above techniques have a common limitation. The do not encode semantics! \n",
        "This means that the vector for `amazing` is completely disimilar from the vector of `great` and the of vector `Laussane`. \n",
        "Can we do better?\n",
        "The answer is yes! Enter, word embeddings. \n",
        "Dense word representation, that can encode the meaning! \n",
        "\n",
        "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/word2vec.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNJ8gO8IKqQ6",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# For more information of TSNE: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "# For more information on GloVe: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "#vectors = open('../data/glove_excerpt.txt').read().strip().split('\\n')\n",
        "vectors = open(workdir + '/data/glove_excerpt.txt').read().strip().split('\\n')\n",
        "vectors = {line.split()[0]:np.array(line.split()[1:]).astype(float) for line in vectors}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pGquTwMSKqRB",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Let's visualize this, using TSNE, a methods that can reduce the dimensionality of the vectors\n",
        "labels = list(vectors.keys())\n",
        "tokens = list(vectors.values())\n",
        "\n",
        "tsne_model = TSNE(perplexity=1.5, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "x = new_values[:,0]\n",
        "y = new_values[:,1]\n",
        "\n",
        "plt.figure(figsize=(7, 6)) \n",
        "for i in range(len(x)):\n",
        "    plt.scatter(x[i],y[i])\n",
        "    plt.annotate(labels[i],\n",
        "                 xy=(x[i], y[i]),\n",
        "                 xytext=(5, 2),\n",
        "                 textcoords='offset points',\n",
        "                 ha='right',\n",
        "                 va='bottom')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lnoq9W0bKqRH"
      },
      "source": [
        "This is a great result for several reasons:\n",
        "- families of similar words are close between them\n",
        "- Some of them encode some syntax (magnificent and amazing) need similar vectors to approach their adverbs! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1gAGaXiss1_",
        "colab_type": "text"
      },
      "source": [
        "## Learning Representations with Deep-NN Models\n",
        "\n",
        "More recently the introduction of deep neural models for building text representations provided us with capabilities of better language understading and subsequently solve easier text related tasks. Specifically, we can distinguish the so-called Transformers in three different classes with respect to the objective they optimize for: \n",
        "\n",
        "- Language Model: estimate the probability of a word given previous words.\n",
        "- Machine Translation: in a sequence mode predict the words in the target sentence.\n",
        "- Masked Language Model: predict the masked token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgaODSt1ss2A",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/lm_models.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnaY1-VKqRJ"
      },
      "source": [
        "# An Introduction to Cross-Lingual Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bHkm1AgiKqRN"
      },
      "source": [
        "\n",
        "Thus far we dealt with embeddings created from English corpus (mono-lingual embedding).\n",
        "\n",
        "In this section we introduce **Cross-Lingual Embedding** which **jointly create ONE embedding for multiple languages**.\n",
        "\n",
        "What this means? It means that **similar words(or sentence, documents) regardless of the language, have similar vector representation**.\n",
        "\n",
        "There are a lot of useful tasks you can do with this, like grouping (or clustering) words from different language by their meanings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gFFcdUPDKqRO"
      },
      "source": [
        "## Cross-Lingual Word Embedding \n",
        "\n",
        "Let's first see see how this work for **Word Embedding**.\n",
        "\n",
        "We will use a pre-trained **ConceptNet** multilingual embeddings for English and French. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ywh0cL6RKqRQ",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from src.utils import load_embeddings,emb2numpy\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ooB-uKbhKqRX",
        "colab": {}
      },
      "source": [
        "# Create lists of example English and French words\n",
        "\n",
        "english_words = [\"room\",\"hotel\",\"towel\",\"book\",\"coffee\",\"chair\",\"glass\",\"pen\",\"shoe\",\"two\",\"amazing\"]\n",
        "french_words = [\"hôtel\",\"chambre\",\"livre\",\"café\",\"chaise\",\"serviette\",\"verre\",\"stylo\",\"chaussure\",\"deux\",\"fantastique\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBPO68KpM1ZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained embedding. This may take a time\n",
        "\n",
        "en_emb = load_embeddings(path=workdir + \"concept_net_1706.300.en\", dimension=300,skip_header=False,vocab=english_words)\n",
        "fr_emb = load_embeddings(path=workdir + \"concept_net_1706.300.fr\", dimension=300,skip_header=False,vocab=french_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YsY5JTS2KqRd",
        "colab": {}
      },
      "source": [
        "# Put the vectors in arrays for processing\n",
        "\n",
        "words_en,V_en = emb2numpy(en_emb)\n",
        "words_fr,V_fr = emb2numpy(fr_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0xuDxZnKqRh",
        "colab": {}
      },
      "source": [
        "vectors = np.concatenate((V_en,V_fr))\n",
        "all_words  = words_en+words_fr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4_T_YWMMKqRl",
        "colab": {}
      },
      "source": [
        "# We project the 300d vectors to a 2d space for visualization\n",
        "V_umap = UMAP(n_neighbors=3,min_dist=0.6).fit_transform(vectors)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qd0I1LtSKqRo",
        "colab": {}
      },
      "source": [
        "sns.set_context(\"talk\")\n",
        "\n",
        "fig= plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
        "for i, word in enumerate(all_words):\n",
        "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "12fc6__NKqRr"
      },
      "source": [
        "Observe that **the words from the different languages but similar meanings are clustered** in the embedding projection. This means that this embedding somehow learned that words' semantic meanings across languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f76Eg6DlbyVm",
        "colab_type": "text"
      },
      "source": [
        "## How word embeddings are created\n",
        "\n",
        "So this looks a bit like a magic. How do we create such embeddings? \n",
        "\n",
        "---\n",
        "\n",
        "There are more than one way to create cross-lingual embedding; one of a fairly simple but effective way is to learn **projection (or alignment) function between the languages** with **supervised training**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5V3bPXhdKqRs"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "1. Train word embeddings separately.\n",
        "2. Create some sort of supervised training data using some aligned corpus (for example, bilingual dictionary).\n",
        "3. Train an alignment function that projects the vectors to a common space.\n",
        "\n",
        "\n",
        "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/alignment.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vLQpTmnCKqRt"
      },
      "source": [
        " \n",
        "---\n",
        "\n",
        "For the case of ConceptNet, projection function is a simple **linear projection function that minimize the vector distance between the two words with the same meaning**: \n",
        "\n",
        "* First create word pairs using the dictionary. There are as many (X, Y) pair as there are words in the dictionary.\n",
        "\n",
        "* Then find the coefficient W which yields smallest total distance for all the pair.\n",
        "\n",
        "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/bilingual_alignement.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5uZmRruKqRv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A few comments:\n",
        "\n",
        "* Type of alignment function, and type of supervision are active area of research. \n",
        "* Type of supervision can vary from parallel sentences (for example, parallel translation) to much cheaper signals like bilingual dictionaries.\n",
        "* Other recent methods do not require any seed dictionaries and induce in an iterative procedure one that is used to learn the projections. \n",
        "\n",
        "For a comprehensive study one can refer to [ Ruder et al., A Survey Of Cross-lingual Word Embedding Models](https://arxiv.org/abs/1706.04902)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00QpdcTcaiC_",
        "colab_type": "text"
      },
      "source": [
        "Any question? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z7JSrGcJKqRy"
      },
      "source": [
        "## Cross-Lingual Sentence Embeddings\n",
        "\n",
        "* Word embedding is cool, but it's a bit unsettling\n",
        "* It ignores **word order**, but we know that order of words could change meanings. \n",
        "* It does not consider that **words change their meanings** depends on the contexts\n",
        "* In short, they could miss salient information as they neglect **linguistic dependencies**. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47xjiIKFy47P",
        "colab_type": "text"
      },
      "source": [
        "### LASER Sequence Encoder\n",
        "\n",
        "* Can we create embeddings for **ordered sequence of words**? such as sentence or document.\n",
        "\n",
        "* Some type of NN models, like LSTM (a type of RNN) or models with attention, has been widely used for prediction over sequencial data. They turned out to be good models for Embedding as well (ELMO, BERT) \n",
        "\n",
        "* In recent year Facebook AI released a **multi-lingual sentence encoder** trained over sentences from 93 languages. [LASER](https://github.com/facebookresearch/LASER) (Language-Agnostic Sentence Representations) \n",
        "\n",
        "* What this means? Now we can create **dense vector representation of any sentence in any (of 93) languages in common embedding space, with one encoder**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvAoryhu4MEF",
        "colab_type": "text"
      },
      "source": [
        "## How sentence encoders are trained\n",
        "\n",
        "**Caveat:** Cross-lingual training is in general very labor intensive. It requires substantial parallel corpora (LASER used over 223 million parallel sentences) and resource (LASER took over 5 days). Luckily, many of pre-trained embeddings are open source and readily available.\n",
        "\n",
        "As a ML practioner you would not often train one from scratch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFJeyzIHKqRz"
      },
      "source": [
        "Here is how it work in a nutshell.\n",
        "\n",
        "This schematic graph of LASER taken from its publication from TACL 2017 \n",
        "\n",
        "* This is a Deep Recurrent Neural Network.\n",
        "* It consistes of **2 chambers**, one for **encoding**, on the left and the other for **decoding**, on the right. \n",
        "* Inputs (say English sentence), bottom left, goes first into the a layer of Byte pair encoder (BPE - its kind of very sophisticated tokenizer) then fed into 5 layers of Bi-directional LSTM (BiLSTM), then aggregated at the top to fit into a fixed size encodeing. \n",
        "* The encoded sentences then fed into the decoder where the output sentence (French, for example) are generated one token at a time. (note that a part of the input to the decoder is the output from the previous position) \n",
        "* Note that there is horizontal arrows linking token as well as the vertical arrow to the next network layer. \n",
        "* Once model is fully trained, the Encoder (and max-pulling step) are used for sentence embedding purpose.\n",
        " \n",
        "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/laser.png?raw=1)\n",
        "\n",
        "This neural network is rather complicated function, but in a split it's doing kind of same things as the cross-lingual word embedding. \n",
        "\n",
        "There, embedding was created by learning a projection function which minimize the distance between the words of same meaning.\n",
        "\n",
        "Here, embedding is created by learning a generation function which generate a text of same meaning; and instead of dictionary, it used aligned sentences for training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwhsUwnNKqR1"
      },
      "source": [
        "## Hands-on experiment: clustering sentences\n",
        "\n",
        "Let's use LASER and see how well can embed a few parallel sentences in English, French and Greek. For this, we will use the Doc2Laser class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFLwkpPrKqR2",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from src.models import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LUqdUD-eKqR5",
        "colab": {}
      },
      "source": [
        "print(Doc2Laser.__doc__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Ts_HfX3KqR9",
        "colab": {}
      },
      "source": [
        "# English\n",
        "en_sentences = [\"This is a nice hotel.\",\n",
        "                \"The bathroom was clean\",\n",
        "                \"The dog is brown\",\n",
        "                \"I will call you\",\n",
        "               \"Not very far from the center\"]\n",
        "\n",
        "# define a transformer -- put cpu=False to turn on GPU option. Goes faster\n",
        "doc2laser_transformer = Doc2Laser(\"en\")\n",
        "\n",
        "# Get the representation of the sentences\n",
        "X_en = doc2laser_transformer.transform(en_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J0n9jzqOKqSC",
        "colab": {}
      },
      "source": [
        "# French\n",
        "fr_sentences = [\"Celui-ci était un hôtel magnifique\",\n",
        "                \"La salle de bain était propre\",\n",
        "                \"Le chien est brun\",\n",
        "                \"Je t'appelle\",\n",
        "               \"Pas très loin du centre\"]\n",
        "\n",
        "# Change the language in the transformer\n",
        "doc2laser_transformer.set_params(lang=\"fr\")\n",
        "X_fr = doc2laser_transformer.transform(fr_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PssbZDwcKqSF",
        "colab": {}
      },
      "source": [
        "# Greece\n",
        "\n",
        "gr_sentences = [\"Το ξενοδοχείο ήταν υπέροχο\",\n",
        "                \"Η τουαλέτα ήταν καθαρή\",\n",
        "                \"Ο σκύλος είναι καφέ\",\n",
        "                \"Σε παίρνω τηλέφωνο\",\n",
        "                \"Όχι πολύ μακριά από το κέντρο\"]\n",
        "# Change the language in the transformer\n",
        "doc2laser_transformer.set_params(lang=\"el\", cpu=False)\n",
        "X_gr = doc2laser_transformer.transform(gr_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_z2UrMGEKqSJ"
      },
      "source": [
        "Let's project the sentence representations now in a 2d space and check if the parallel sentences in the three languages are close."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6LZS5AS-MPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V_umap = UMAP(n_neighbors=5,min_dist=0.2).fit_transform(np.concatenate((X_en,X_fr)))\n",
        "\n",
        "fig= plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
        "for i, word in enumerate(en_sentences+fr_sentences):\n",
        "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47b3QnaS-UA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V_umap = UMAP(n_neighbors=5,min_dist=0.2).fit_transform(np.concatenate((X_en,X_gr)))\n",
        "\n",
        "fig= plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
        "for i, word in enumerate(en_sentences+gr_sentences):\n",
        "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvJFV-RnKqSj",
        "colab": {}
      },
      "source": [
        "V_umap = UMAP(n_neighbors=5,min_dist=0.2).fit_transform(np.concatenate((X_en,X_fr,X_gr)))\n",
        "\n",
        "fig= plt.figure(figsize=(12,6))\n",
        "\n",
        "#plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
        "plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
        "for i, word in enumerate(en_sentences+fr_sentences+gr_sentences):\n",
        "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pjeUc14SKqSq"
      },
      "source": [
        "We can observe that the parallel sentences are close to the embedding space which means that the model can capture the semantic in a single latent multi-lingual space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4gfSqOtKqSs"
      },
      "source": [
        "***Exercise:*** Try to add few more parallel sentence in other languages and project them with the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJKR9eXk5ZLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_words(en_sentences, gr_sentences, language):\n",
        "\n",
        "  doc2laser_transformer = Doc2Laser(\"en\")\n",
        "  X_en = doc2laser_transformer.transform(en_sentences)\n",
        "  \n",
        "  doc2laser_transformer = Doc2Laser(language)\n",
        "  X_gr = doc2laser_transformer.transform(gr_sentences)\n",
        "\n",
        "  V_umap = UMAP(n_neighbors=5,min_dist=0.2).fit_transform(np.concatenate((X_en,X_gr)))\n",
        "  fig= plt.figure(figsize=(12,6))\n",
        "\n",
        "  plt.scatter(V_umap[:, 0], V_umap[:, 1])\n",
        "  for i, word in enumerate(en_sentences + gr_sentences):\n",
        "    plt.annotate(word, xy=(V_umap[i, 0], V_umap[i, 1]))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2SGzQ_V69zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_sents = [\"This is a nice hotel.\",\n",
        "                \"The bathroom was clean\",\n",
        "                \"The dog is brown\",\n",
        "                \"I will call you\",\n",
        "               \"Not very far from the center\"]\n",
        "gr_sents = [\"Το ξενοδοχείο ήταν υπέροχο\",\n",
        "                \"Η τουαλέτα ήταν καθαρή\",\n",
        "                \"Ο σκύλος είναι καφέ\",\n",
        "                \"Σε παίρνω τηλέφωνο\",\n",
        "                \"Όχι πολύ μακριά από το κέντρο\"]\n",
        "lang = \"el\"\n",
        "map_words(en_sents, gr_sents, lang)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2TiV2i9dKqS2"
      },
      "source": [
        "# Cross-lingual Document Classification\n",
        "\n",
        "One of the best uses of Cross-lingual Embedding is for performing so-called **Transfer Learning** across different languages:\n",
        "\n",
        "- In practical NLP tasks, you often have very limited access to training data in your target languages, even when there are plenty of data in other language (English, for example).\n",
        "\n",
        "- Multi-lingual embedding is used as a mean for transferring knowledge from one language to another \n",
        "\n",
        "- By encoding the data using Multi-lingual embedding we can use resource-rich languages to train models then directly apply them to resource-deprived languages.\n",
        "\n",
        "- Two types of transfer learning we cover in this workshop: **Zero-shot Learning** and **Few-shot Learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dp5AQlhRKqS5"
      },
      "source": [
        "## Hands-on experiment: Zero-shot Learning \n",
        "\n",
        "Zero-shot learning is applicable when:\n",
        "\n",
        "- **You have labeled training documents in one language (source documents)** \n",
        "- **You have no training data in the language in which you wish to classify documents (target documents)**\n",
        "\n",
        "In the following demo, we will do this looking impossible task by training a **Cross-Lingual Sentiment Classifiers** via Zero-shot transfer learning. This is pretty simple two steps:\n",
        "\n",
        "* Encode the training text with (some) Cross-lingual embedding \n",
        "* Train a classifier with encoded training data\n",
        "* Encode the test text with the same embedding then run prediction.\n",
        "\n",
        "Let's start.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kBOdp4ohzfN",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Classification revisited.\n",
        "\n",
        "Recall that Sentiment Classification is: \n",
        "\n",
        "- A type of Text Classification task\n",
        "- Input (x): a text span\n",
        "- Output (y): a category (\"Positive\" or \"Negative\")\n",
        "- ML Goal: Train a function that classify input to the correct sentiment  𝑓(𝑥)→𝑦 \n",
        "\n",
        "For the sake for experiment, let's say **we want a model to classify Spanish** documents, but **have only English training data.**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We start with loading data set from the [SemEval](http://alt.qcri.org/semeval2015/) Sentiment Analysis Workshop, **training data in English**, and **test data in Spanish.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0X2djheGKqS8",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from collections import Counter\n",
        "from src.models import *\n",
        "from src.utils import *\n",
        "from src.dataset import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q21T9mlMKqTM",
        "colab": {}
      },
      "source": [
        "# dataset = Dataset(\"../data/raw/\",\"en\", \"es\")\n",
        "dataset = Dataset(workdir + \"data/raw/\",\"en\", \"es\")\n",
        "\n",
        "dataset.load_data()\n",
        "#To check the arguments of the function\n",
        "#print(dataset.load_cl_embeddings.__doc__)\n",
        "dataset.load_cl_embeddings(workdir,300,False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QT-n2F8HKqTu",
        "colab": {}
      },
      "source": [
        "# Plot the counts on the classes for the source language\n",
        "sns.countplot(dataset.y_train,order=[\"negative\",\"positive\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4pXuQPcpKqT6",
        "colab": {}
      },
      "source": [
        "# And for the Spanish dataset\n",
        "sns.countplot(dataset.y_test,order=[\"negative\",\"positive\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyY8IZ3uKqT_"
      },
      "source": [
        "Observe that the datasets are unbalanced as we have much more positive comments that negative ones. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Next, we establish **two baselines**. First one is a simple **majority win** (also called \"dummy classification\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w56E2_mvKqUC",
        "colab": {}
      },
      "source": [
        "# Let's keep the scores of all the expriments in a table\n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"Model\", \"f-score\"]\n",
        "\n",
        "# Majority Class\n",
        "pipeline = Pipeline([('vectorizer', CountVectorizer()), \n",
        "                     ('classifier', DummyClassifier(\"stratified\"))])\n",
        "runner = Runner(pipeline, dataset)\n",
        "score = runner.eval_system()\n",
        "x.add_row([\"Dummy\", format_score(score)])\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bmYK4Ks7KqUR"
      },
      "source": [
        "Second baseline is a **Logistic Regression with unigram count features**. \n",
        "\n",
        "Note that training data are in English, so quite a few features are absent in the test (Spanish) data. So this is quite a naive approach. \n",
        "\n",
        "That said, We expect that in cases the  languages share a part of the vocabulary (for example in Latin languages) this approach can potentially give descent results. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f2s40EwIKqUh",
        "colab": {}
      },
      "source": [
        "# Logistic Regression on words\n",
        "pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True)), \n",
        "                     ('classifier', LogisticRegression(solver=\"lbfgs\"))])\n",
        "runner = Runner(pipeline, dataset)\n",
        "score = runner.eval_system()\n",
        "x.add_row([\"LR unigrams\",format_score(score)])\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ihbev0Ioe-Q",
        "colab_type": "text"
      },
      "source": [
        "Not too bad considering this approach basically oblivious to the language difference.\n",
        "\n",
        "We used unigrams for this exercise, but more involved n-gram features (bigram, trigram, and character-gram) can be tried (see Exercise) Character-gram in particular may improve the score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v8J4jRfsKqU0"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "Let's now use **cross-lingual word embeddings** to create a **cross-lingual classifier** with zero-shot fashion, and see if it come out better.\n",
        "\n",
        "Since the embedding is at word level, we need some way to create a vector representation from the words in the document. Here is one simple (but effective) approach: \n",
        "\n",
        "In the diagram below, the original document is represented as a binary vector with 3 words. \n",
        "\n",
        "1. Convert the document in 1-hot encoding\n",
        "2. Multiplied the vector with the embedding matrix of size $V\\times d$, pulling out the 3 vectors of size $d$. \n",
        "3. Aggregate then average vectors\n",
        "\n",
        "![](https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020/blob/master/data/images/vec_average.png?raw=1)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We repeat this operation for each document. We then train a Logistic Regression on the training set (English) and evaluate on the test set (Spanish).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SGsGu2WOKqU_",
        "colab": {}
      },
      "source": [
        "for name, myclf in zip(['Knn-nBow', 'LR-nBow'],[KNeighborsClassifier(n_neighbors=2), LogisticRegression(C=10, solver=\"lbfgs\")]):\n",
        "\n",
        "    avg_baseline = nBowClassifier(myclf,dataset.source_embeddings,dataset.target_embeddings)\n",
        "\n",
        "    pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True,vocabulary=dataset.vocab_)), \n",
        "                         ('classifier', avg_baseline)])\n",
        "\n",
        "    runner = Runner(pipeline, dataset)\n",
        "    x.add_row([name, format_score(runner.eval_system())])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQTx_vhyKqVI",
        "colab": {}
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cRzpaUeNKqVe"
      },
      "source": [
        "This is much better results than the previous two. Converting to pan-language semantic representation clearly help, despite somewhat crude method in approximating sentence (just averaging)\n",
        "\n",
        "---\n",
        "\n",
        "Finally, let's use **cross-lingual sentence encoding (LASER)** , then train the classifiers. For the sake of comparisons, we keep the same training parameters/framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GH3_g755KqVg",
        "colab": {}
      },
      "source": [
        "for name, myclf in zip(['Knn-laser', 'LR-laser'],[KNeighborsClassifier(n_neighbors=2), LogisticRegression(C=10, solver=\"lbfgs\")]):\n",
        "    laser_clf = LASERClassifier(myclf, dataset.source_lang, dataset.target_lang)\n",
        "    pipeline = Pipeline([(\"doc2laser\",Doc2Laser()),('classifier', laser_clf)])\n",
        "    pipeline.set_params(doc2laser__lang=dataset.source_lang)\n",
        "    pipeline.fit(dataset.train,dataset.y_train)\n",
        "    runner = Runner(pipeline, dataset)\n",
        "\n",
        "    pipeline.set_params(doc2laser__lang=dataset.target_lang)\n",
        "    x.add_row([name, format_score(runner.eval_system(prefit=True))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wyBme3i3KqVr",
        "colab": {}
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jamZepetKqVy"
      },
      "source": [
        "We observe that the zero-shot learning using LASER representations achieve the best results in this pair of languages (English and Spanish). \n",
        "\n",
        "***Exercises:*** \n",
        "* Try higher dependency n-grams and/or character gram to devise strong baseline.\n",
        "* Use other pairs of languages and see the performance. For example, you can try to transfer from more distant languages like Russian.\n",
        "* Write a function in order to calculate all the pairs of (source, target) languages and compare the results.\n",
        "* Tune the classifier or use other type of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMKJFj6NKqV1"
      },
      "source": [
        "## Few-shot Learning\n",
        "\n",
        "Similar to Zero-shot learning, Few-shot learning is applicable when you have language resource constrains, with very slight difference: \n",
        "\n",
        "- **You have A LOT of labeled training documents in one language (source documents)** \n",
        "- **You have SMALL training data in the language in which you wish to classify documents (target documents)**\n",
        "\n",
        "Steps for reating a cross-lingual classifier in Few-shot learning is similar:\n",
        "\n",
        "* Encode ALL (both soure and target) training text with (some) Cross-lingual embedding \n",
        "* Train a classifier with all the encoded training data, regardless of the language \n",
        "* Encode the test text with the same embedding then run prediction.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uag10IGUzLtf",
        "colab_type": "text"
      },
      "source": [
        "### Why few-shot transfer learning works?\n",
        "\n",
        "This sounds rather raidcal, right? \n",
        "\n",
        "However, recent empirical researches demonstrated this strategy works over and over.  \n",
        "\n",
        "In a nutshell, the (larger) training data from other languages give some “scaffold” to push the decision boundary to generally good region. \n",
        "\n",
        "Note that this is only possible because the data are all in common language representation to increase the training data.   \n",
        "\n",
        "![Few Shot Learning](https://upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png)\n",
        "\n",
        "* Above: Too many possible ways to draw decision plane.\n",
        "* Below: Other data guide decision planes to the in general optimal place; The in-language data would add finer discrimination (hopefully)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41Yq3su1FkVX",
        "colab_type": "text"
      },
      "source": [
        "## Hands-on experiment: Few-shot learning \n",
        "\n",
        "Let’s continue the sentiment classification task, but this time we will train **classifier for Russian** using Few-shot Learning. \n",
        "\n",
        "We have done the first step for you already.\n",
        "We have encoded the training data in six languages (English, Dutch, Spanish, Russian, Arabic and Turkish) in LASER. Every sentence is represented by a 1024 dimensional vector.\n",
        "\n",
        "We will do the second step (training the classifier part) together.\n",
        "\n",
        "We will use scikit-learn for training. In addition, we created 3 utility functions for convenience:\n",
        "\n",
        "- ```\n",
        "model_evaluation(model, [languages])\n",
        "```: evaluate the ```model``` over list of ```languages```. Returns [F1](https://en.wikipedia.org/wiki/F1_score) score, and [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
        "- ```load_training_languages([languages])```: Returns ```x_train, y_train```, concatenated features and labels for the languages specified in ```languages```.\n",
        "- ```get_statistics([languages]```: print out data size and split of the languages specified in ```languages```.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Let us start by checking which language has a good sized data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6gF7VvHKqV4",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "sys.path.insert(1, workdir)\n",
        "\n",
        "from src.utils import load_training_languages, model_evaluation, get_statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smyeG_qQSvdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets check which langauge has a good sized data:\n",
        "\n",
        "all_languages = ['en','es','nl','ru','ar','tr']\n",
        "get_statistics(all_languages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRl9wf5OFD7o",
        "colab_type": "text"
      },
      "source": [
        "There are only 2,542 unbalanced data in Russian. This is faily small for text classification.\n",
        "\n",
        "Nonetheless, let's train a [Logistic Regression](https://fr.wikipedia.org/wiki/R%C3%A9gression_logistique) (a linear classifier) on the Russian training data, and see how it does. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OBUVG8CnKqWG",
        "colab": {}
      },
      "source": [
        "x_train,y_train = load_training_languages(['ru'])\n",
        "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
        "_ = model_evaluation(lr, ['ru'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "78896zX5KqWV"
      },
      "source": [
        "The overall performance is not fantastic. (you should get around 0.70 F1 score.) Could we do better? \n",
        "\n",
        "Lets add other langauge with larger data set and see how it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b_aDaev8KqWg",
        "colab": {}
      },
      "source": [
        "x_train,y_train = load_training_languages(all_languages)\n",
        "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
        "_ = model_evaluation(lr, ['ru'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "csU-_bJfKqWt"
      },
      "source": [
        "The F1 score has improved by 0.1! Quite impressive.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's do something even more challening. \n",
        "\n",
        "Turkish has even smaller data set than Russian. Let's see how we do with this\n",
        "\n",
        "We first train a classifier with the same langauge to set the baseline:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVW_rO_dKqWz",
        "colab": {}
      },
      "source": [
        "# First, get the baseline performance using in-domain data: \n",
        "# Train on Turkish and Test on Turkish.\n",
        "\n",
        "x_train,y_train = load_training_languages(['tr'])\n",
        "lr = LogisticRegression(C = 10,random_state = 1).fit(x_train,y_train)\n",
        "_ = model_evaluation(lr, ['tr'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C3zZQnaNKqW9"
      },
      "source": [
        "The F1 score is now quite low :( \n",
        "\n",
        "Small data size tends to leads to poor performance. Also, Turkish has complex rich morphology thus much difficult to lean.\n",
        "\n",
        "Let's use all available languages to improve our model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5qs4LvaKqXA",
        "colab": {}
      },
      "source": [
        "\n",
        "x_train,y_train = load_training_languages(all_languages)\n",
        "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
        "_ = model_evaluation(lr, ['tr'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pQf1QyeGKqXG"
      },
      "source": [
        "Not much improvement... Maybe another combination of languages leads to different results?\n",
        "\n",
        "What happen if we remove spanish and russian from the training set?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LnvlGoxyKqXH",
        "colab": {}
      },
      "source": [
        "\n",
        "x_train,y_train = load_training_languages(['ar','tr','nl','en'])\n",
        "lr = LogisticRegression(C = 10,max_iter = 200,random_state = 1).fit(x_train,y_train)\n",
        "_ = model_evaluation(lr, ['tr'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXdtzQgwcv-S",
        "colab_type": "text"
      },
      "source": [
        "Better! Apparently spanish and russian were perturbing the model for turkish language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JvX_liTERnBB"
      },
      "source": [
        "\n",
        "***Exercises:*** \n",
        "\n",
        "* Test your all-language model against various test languages. How their performance differs? \n",
        "* Repeat the experiment with different language combinations. Could we imagine a more systematic source language selection to optimize performance on specific target language?\n",
        "* Try different target languages. Which language is difficult? \n",
        "* Try classifier other than linear model. Is it better or worse? What can we conclude from the above results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cWr_CCX7m1M",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "[1. Ruder et al., A Survey Of Cross-lingual Word Embedding Models](https://arxiv.org/abs/1706.04902)\n",
        "\n",
        "[2. Artexte and Schwenk, Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://arxiv.org/abs/1812.10464)\n",
        "\n",
        "[3. Lena Voita et al., Evolution of Representations in the Transformer](https://arxiv.org/abs/1909.01380)\n",
        "\n",
        "[4. Balikas and Partalas, Wasserstein distances for evaluating cross-lingual embeddings](https://arxiv.org/abs/1910.11005)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7H_ehVoIhc8",
        "colab_type": "text"
      },
      "source": [
        "## extra slides\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmnE728DETdK",
        "colab_type": "text"
      },
      "source": [
        "Exercise suggestion for Few-shot learning \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Until now we have used Logisitic Regression. However more complex models, such as [multi layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) \n",
        "\n",
        "or [extreme gradient boosting](https://en.wikipedia.org/wiki/XGBoost) (xgboost) are obviously possible.\n",
        "\n",
        "Lets try predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7pcPVEvNKqXK",
        "colab": {}
      },
      "source": [
        " from sklearn.neural_network import MLPClassifier\n",
        " mlp = MLPClassifier(solver='lbfgs', \n",
        "                     hidden_layer_sizes=(16),\n",
        "                     activation = 'relu',\n",
        "                     alpha=1e-3,\n",
        "                     max_iter = 50,\n",
        "                     early_stopping =True,\n",
        "                     validation_fraction = 0.2, \n",
        "                     random_state=1)\n",
        " _ = model_evaluation(mlp.fit(x_train,y_train),['ru'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yyiIGc05KqXR",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "boost = xgb_model = xgb.XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    max_depth =5, \n",
        "    random_state=42)\n",
        "_ = model_evaluation(boost.fit(x_train,y_train),['ru'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}